# `get_website` Tool ‚Äì Architecture & Implementation Details\n\n## Overview\n`get_website` is the public entry‚Äëpoint that a client calls via the JSON‚ÄëRPC `tools/call` method with the name `get_website`. It returns a markdown‚Äëformatted string containing the page title, URL, access timestamp, excerpt, and the extracted content (or an error message). The implementation is split across several components:\n\n1. **Input handling & URL normalization** ‚Äì ensures a scheme is present.\n2. **Domain‚Äëbased rate limiting** ‚Äì prevents excessive requests to the same host.\n3. **Cache lookup & validation** ‚Äì fast return of recent results.\n4. **Fetching & content extraction** ‚Äì performed by the private helper `_get_website_content_cached`.\n5. **Citation generation** ‚Äì optional emission of citation events.\n6. **Result formatting** ‚Äì assembles a user‚Äëreadable markdown block.\n7. **Error handling** ‚Äì propagates clear, typed exceptions.\n\n---\n\n## 1. URL Normalization\n```python\nparsed_url_input = urlparse(url)\nif not parsed_url_input.scheme:\n    url = \"https://\" + url\n    logger.info(f\"URL '{url}' was missing a scheme. Prepended https://.\")\n```\n* If the caller supplies a URL without `http://` or `https://`, the function prepends `https://` and logs the adjustment.\n* The normalized URL is used for all subsequent steps.\n\n---\n\n## 2. Rate Limiting\nA per‚Äëdomain token‚Äëbucket is implemented by the `RateLimiter` class (lines 63‚Äë138).\n* **Construction** ‚Äì uses the configuration values `RATE_LIMIT_REQUESTS_PER_MINUTE` and `RATE_LIMIT_TIMEOUT_SECONDS` from `Tools.Valves`.\n* **`can_request(url)`** ‚Äì returns `True` if the domain has not exceeded the allowed request count in the sliding window; otherwise `False`.\n* **`get_remaining_time(url)`** ‚Äì computes the seconds to wait until the next request is permitted.\n\nIn `get_website`:\n```python\nrate_limiter = RateLimiter(\n    requests_per_minute=self.valves.RATE_LIMIT_REQUESTS_PER_MINUTE,\n    timeout_seconds=self.valves.RATE_LIMIT_TIMEOUT_SECONDS,\n)\nif not rate_limiter.can_request(url):\n    wait_time = rate_limiter.get_remaining_time(url)\n    error_msg = (\n        f\"Rate limit exceeded for domain {parsed_url_input.netloc}. \"\n        f\"Try again in {wait_time:.1f} seconds.\"\n    )\n    logger.warning(error_msg)\n    await self.emitter.emit(status=\"error\", description=f\"‚ùå {error_msg}\", done=True)\n    raise RateLimitExceededError(error_msg)\n```\nIf the limit is hit, a `RateLimitExceededError` is raised after emitting an error event.\n\n---\n\n## 3. Cache Lookup & Validation\nA global `website_cache` (`TTLCache`) is created during `Tools` initialization (line 59). The cache stores the **raw result dict** (including a `soup` object, title, content, etc.)\n* **Validation** ‚Äì performed by `CacheValidator.is_valid(cached_result, max_age_minutes)` (lines 140‚Äë172). It checks the presence of a `date_accessed` field and verifies the age does not exceed `CACHE_MAX_AGE_MINUTES`.\n* **Cache hit path**:\n```python\nif cached_result and CacheValidator.is_valid(cached_result, self.valves.CACHE_MAX_AGE_MINUTES):\n    await self.emitter.emit(\"üìë Content retrieved from cache\", step_number=3)\n    result_data = cached_result.copy()\n    soup = None\n    error = result_data.get(\"error\")\n```\n* **Stale entry** ‚Äì if the entry exists but is older than the configured max age, it is refreshed (log message on line 822).\n\n---\n\n## 4. Fetching & Content Extraction (`_get_website_content_cached`)\n### 4.1 Entry Point\n`get_website` calls the private async method:\n```python\nresult_data = await self._get_website_content_cached(url)\n```\n### 4.2 Steps Inside `_get_website_content_cached`\n1. **Reddit URL rewrite** ‚Äì `HelperFunctions._modify_reddit_url(url)` (lines 224‚Äë230) rewrites `https://www.reddit.com/...` to `https://old.reddit.com/...` for better extraction.\n2. **HTTP request** ‚Äì performed with `self.client.get(url_to_fetch, timeout=120)`. The client is an `httpx.AsyncClient` with a custom User‚ÄëAgent.\n3. **MIME type detection** ‚Äì `filetype.guess(raw_content)` determines if the response is a PDF.\n4. **PDF handling** (lines 757‚Äë762):\n   * Load PDF via `pymupdf.open(stream=raw_content, filetype=\"pdf\")`.\n   * Convert to markdown with `pymupdf4llm.to_markdown(doc)`.\n   * Truncate to the configured word limit (`PAGE_CONTENT_WORDS_LIMIT`).\n   * Generate an excerpt via `HelperFunctions.generate_excerpt`.\n   * Title is set to a static string *\"A PDF document converted to Markdown\"* and `soup` is `None`.\n5. **HTML handling** (lines 769‚Äë777):\n   * Parse with `BeautifulSoup(html_content, \"html.parser\")`.\n   * Extract the `<title>` tag, normalise Unicode, strip emojis.\n   * Extract main text with `HelperFunctions.format_text_with_trafilatura(html_content, self.valves.TRAFILATURA_TIMEOUT)`.\n   * Apply truncation and excerpt generation similarly to PDF.\n6. **Return dict** (lines 778‚Äë782) containing:\n   * `title`, `url`, `content` (truncated), `excerpt`, `date_accessed` (UTC ISO), `soup` (BeautifulSoup object or `None`), `error: None`.\n7. **Error handling** ‚Äì three `except` blocks translate HTTP status errors, request errors, and generic exceptions into a dict with `error` and `date_accessed` fields, and `soup: None`.\n\n---\n\n## 5. Citation Generation\nIf the configuration flag `CITATION_LINKS` is `True` **and** a `soup` object is present (i.e., the result was HTML, not PDF), the tool emits a citation event after successful fetch:\n```python\nif self.valves.CITATION_LINKS and soup:\n    await self.emitter.emit(\"üìö Generating citations\", step_number=6)\n    await self.emitter.emit_citation(soup, result_data)\n```\n`MCPEventEmitter.emit_citation` extracts metadata (author, publish date, source) and sends a `tool/event` notification of type `citation`.\n\n---\n\n## 6. Result Formatting\nThe final markdown string is built from a list of parts (lines 845‚Äë856):\n```python\noutput_parts = [\n    f\"**Title:** {result_data.get('title', 'N/A')}\",\n    f\"**URL:** {result_data.get('url', 'N/A')}\",\n    f\"**Date Accessed:** {result_data.get('date_accessed', 'N/A')}\",\n    \"\\n**Excerpt:**\",\n    result_data.get('excerpt', ''),\n    \"\\n\\n**Content:**\",\n    result_data.get('content', '')\n]\nif result_data.get(\"error\"):\n    output_parts.insert(2, f\"**Error:** {result_data.get('error')}\")\nreturn \"\\n\".join(output_parts)\n```\n* Errors (if any) are injected as a bold line after the title.\n* The response is plain markdown; the JSON‚ÄëRPC wrapper then returns it under `content` with `type: \"text\"`.\n\n---\n\n## 7. Error Propagation & Exceptions\n| Exception | Trigger | Handling Path |\n|-----------|---------|---------------|\n| `RateLimitExceededError` | Domain request exceeds `RATE_LIMIT_REQUESTS_PER_MINUTE` | Emitted as an error event, then raised (line 810). |\n| `WebScrapingError` | Any exception inside `_get_website_content_cached` not caught by specific handlers | Logged, error event emitted, then raised (line 835). |\n| `ConfigurationError` | Invalid configuration values (e.g., malformed `SEARXNG_ENGINE_API_BASE_URL`) ‚Äì not directly used by `get_website` but may affect cache/valves. |\n| Generic `MCPServerError` | Unexpected failures in the surrounding RPC loop. |\n\nAll errors are surfaced to the client as a markdown block prefixed with `‚ùå` via the event emitter and also as part of the JSON‚ÄëRPC `result` with `isError: True` when the tool call fails.\n\n---\n\n## 8. Interaction Flow Summary\n```mermaid\nsequenceDiagram\n    participant Client\n    participant Server\n    participant Cache\n    participant RateLimiter\n    participant HTTP\n    participant Emitter\n    Client->>Server: tools/call {name: \"get_website\", arguments:{url}}\n    Server->>Server: normalize URL\n    Server->>RateLimiter: can_request?\n    alt allowed\n        Server->>Cache: get(url)\n        alt cache hit & valid\n            Server->>Server: use cached result\n        else\n            Server->>HTTP: GET (with Reddit rewrite)\n            HTTP-->>Server: response (HTML or PDF)\n            Server->>Server: detect MIME, extract text\n            Server->>Cache: store fresh result\n        end\n        alt CITATION_LINKS && soup exists\n            Server->>Emitter: emit citation event\n        end\n        Server->>Emitter: emit success event\n        Server->>Client: result markdown\n    else\n        Server->>Emitter: emit rate‚Äëlimit error event\n        Server->>Client: JSON‚ÄëRPC error (RateLimitExceededError)\n    end\n```\n---\n\n## 9. Configuration Influence\n| Config Variable | Effect on `get_website` |\n|-----------------|------------------------|\n| `CITATION_LINKS` | Enables/disables citation events. |\n| `CACHE_MAXSIZE` / `CACHE_TTL_MINUTES` | Controls cache capacity and TTL for stored pages. |\n| `CACHE_MAX_AGE_MINUTES` | Maximum age before a cached entry is considered stale and refreshed. |\n| `RATE_LIMIT_REQUESTS_PER_MINUTE` / `RATE_LIMIT_TIMEOUT_SECONDS` | Directly drives the `RateLimiter` thresholds. |\n| `TRAfilatura_TIMEOUT` | Timeout for the Trafilatura text extraction step (HTML only). |\n| `PAGE_CONTENT_WORDS_LIMIT` | Upper bound on the number of words returned in the final content field. |\n| `DESIRED_TIMEZONE` | Not used by `get_website` directly, but influences other tools.\n\n---\n\n## 10. Key Design Decisions\n* **Separate cache validation** ‚Äì ensures stale data does not silently propagate.\n* **Domain‚Äëbased rate limiting** ‚Äì protects both the server and target websites.\n* **PDF first‚Äëclass support** ‚Äì PDFs are detected early via MIME sniffing and converted to markdown, preserving readability.\n* **Reddit URL rewrite** ‚Äì improves extraction quality for Reddit pages.\n* **Event‚Äëdriven progress reporting** ‚Äì the client receives granular status updates (`üîç Accessing URL`, `üìë Content retrieved from cache`, `üìö Generating citations`, etc.).\n* **Graceful degradation** ‚Äì on any HTTP or parsing error the tool returns a clear error message rather than raising an uncaught exception.\n\n---\n\n## 11. Future Enhancements (Ideas)\n1. **Parallel fetches** ‚Äì batch multiple URLs in a single request for efficiency.\n2. **Pluggable extractors** ‚Äì allow swapping Trafilatura for another parser.\n3. **More granular caching** ‚Äì separate PDF and HTML caches with different TTLs.\n4. **Configurable citation granularity** ‚Äì optional inclusion of full HTML source for advanced downstream processing.\n5. **Retry logic** ‚Äì exponential back‚Äëoff for transient network failures.\n\n---\n\n*Document generated by analysis of `mcp_server.py` (lines 720‚Äë860 and supporting sections).*